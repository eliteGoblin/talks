# Why kubernetes

a beginner's guide from microservice perspective  

30 May 2020

Frank Sun
Golang Developer, API Team
elitegoblinrb@gmail.com

## About this session and myself

Challenge in Microservice and comparison between solution with and without K8s

Microservice experience in Udesk

## Roadmap

// TODO: add Roadmap when all finished

## What's not in this session

*  Declarative syntax
*  K8s all terminology in details

// k8s build on docker, solve microservice problems, we need to know docker, k8s, and microservice; each itself is a big topic
// Goal is understand what problems to solve, then understand why k8s

## What is Kubernetes

Kubernetes (“koo-burr-NET-eez”) is originated from a Greek word, κυβερνήτης, meaning “helmsman” or “pilot.” 

Wikipedia    

An open-source container-orchestration system for automating application deployment, scaling, and management.  It aims to provide a "platform for automating deployment, scaling, and operations of application containers across clusters of hosts"

Kubernetes.io    

Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. 

## What is Kubernetes

Key words: 

- Container
- Orchestration, managing
- Automation 
- Scaling, Auto heal

Kubernetes is the your computing cluster's operating system.

// Analogy, let's see the different with traditional OS

## Linux as an OS example

OS manages computer hardware, software resources, and provides common services for computer programs.

// ![20200531115737](https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20200531115737.png)

We use Linux's abstraction to manage machine(single) resources:

-  Disk: File, Directory
-  Runtime: Process: fork, wait, exec, groups
-  Inside Process: Thread: pthread, mutex, locks
-  Network: Sockets

Abstract computer resource(single) as Linux terminology

## Kubernetes as cluster OS

![20200531124003](https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20200531124003.png)

## Kubernetes as cluster OS

Linux as OS provide POSIX API for writing application running on single computer
K8s as OS provide useful API for building microservice system on cluster: 

- **Workloads scheduling**
- **Service discovery, Load balancing**
- **Automated rollouts and rollbacks**
- **Self-healing**
- **Scaling**
- Storage orchestration
- Secret and configuration management

K8s makes building Microservice much easier! 

// Productivity, Small group of people can manage large cluster

## Kubernetes terminology

We need to talk in K8s terminology: 

- Node: a single computer
- **Pod**: basic running unit, like process in Linux 
- **Replicate Set**: group of identical pods(replicas)
- **Service**: reverse proxy/load balancer, like Nginx 
- Deployments
- Daemon Set
- Job
- Secrets and configMap
- Volume

// Pod (unbreakable unit when deploy and run), 约等于contfainer
// It provide useful functions when building micro-service

## Why named Pod?

Pod is a social group of whales, means group of containers

<img src="https://cdn.vox-cdn.com/thumbor/YiSPuNNVjUhKnYPpqzVinwYCRK8=/1400x1400/filters:format(png)/cdn.vox-cdn.com/uploads/chorus_asset/file/11422405/Docker_logo_011.png" alt="drawing" width="600"/>

## K8s architecture

![20200531215220](https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20200531215220.png)

// Human use kubectl to access apiserver

ETCD: Phonebook of all services(imagine you lost all your contact, WFH and slack down)

## K8s architecture

![k8s](https://d33wubrfki0l68.cloudfront.net/7016517375d10c702489167e704dcb99e570df85/7bb53/images/docs/components-of-kubernetes.png)
![architecture](https://www.tutorialspoint.com/kubernetes/images/cluster_architecture.jpg)

## K8s History and Jobs Trend

*  v0.4 20 Nov 2014
*  v1.0  2 Jun 2016
*  v1.18 Now

Between October 2015 and October 2019, the share of Kubernetes job searches increased by **2,125%**.  Kubernetes-related jobs per million also grew by **2,141%** in that same time period,

From: [Kubernetes in 2020](https://www.beseen.com/blog/talent/kubernetes-career-trends/)

Also interesting
[2020 Emerging Jobs Report](https://business.linkedin.com/content/dam/me/business/en-us/talent-solutions/emerging-jobs-report/Emerging_Jobs_Report_U.S._FINAL.pdf)

## K8s Jobs Trend: Employer's Interest

// buyer
From 2018 to 2019, the share of Kubernetes jobs per million rose 53.33%

<img src="https://www.beseen.com/wp-content/uploads/2020/01/Employer-interest-in-Kubernetes-roles-over-time.png" alt="drawing" width="600"/>

## K8s Jobs Trend: Job Seeker's Interest

// seller
From 2018 to 2019, the share of Kubernetes job searches saw a mere .85% gain.

<img src="https://www.beseen.com/wp-content/uploads/2020/01/Job-seeker-interest-in-Kubernetes-roles-over-time.png" alt="drawing" width="600"/>

## Microservice, what and why

Microservice is an architectural style that structures an application as a collection of services that are.

// We need to first understand challenges in microservice then can understand how to compare solution with/without k8s

Small, and Focused on Doing One Thing Well

- Resilience: buckhead, failure doesn’t cascade
- Scaling: Monolithic service has to scale everything together
- Deployment separately
- Technology Heterogeneity
- Optimizing for Replaceability/Refactoring
- Organizational Alignment: smaller team to manage smaller service; Conway's law

// US only need to scale sales/marketing, not much engineer/tech
// Because of one fail part(only if we do it right)
// Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure.

## Challenges of Microservice

- Deployment, Resource planning
- High availability(HA)
- Service Discovery
- Release: Automated rollouts and rollbacks with zero-downtime
- Scaling

// Kubernetes as cluster OS

## API team's Microservice architecture

// What happens behind the scene of Map browser

Intenal service:
*  Areas
*  Transactions

## Containerize and docker

Running software in containers for isolation: Docker in Action, 1.1.3 图?

- **Self-contained, isolation**: isolated process and network
- **Resource requirement**
- Lightweight
- Portable

Container images bundle a program and its dependencies into a single artifact.

//  all of these programs share the same versions of shared libraries on the system
// Isolated process in Linux

// Can just work in a total different environment

// Think it's a light weighted VM, cgroup

// everbright bank story: depend on same version of redis-cli, sql driver
// Isolation: share house and studio, buckhead

## Resource request and limit in docker container

*  Resources: CPU, memory, GPU
*  Requests means reserve, are what the container is guaranteed to get
*  Limits, make sure a container never goes above a certain value
// CPU limit, memory limit

[Request, Limit), or Request <= RuntimeResource < Limit

```go
docker run -it --cpus=".5" ubuntu /bin/bash
```
*  1m CPU = 1 / 1000 Core (mili-core)
*  1m Memory = 1 Medibyte(megabyte)

Generally, you should do resource planning before deploy: specify resource request and limit

// millicores. If your container needs two full cores to run, you would put the value “2000m”
//  If your app starts hitting your CPU limits, Kubernetes starts throttling your container. This means the CPU will be artificially restricted, giving your app potentially worse performance! However, it won’t be terminated or evicted

//  mebibyte 2^20
// , if a container goes past its memory limit it will be terminated.

//  it’s important that the containers have enough resources to actually run

## Demo env

*  Gcloud
*  Kubectl: -v 6, 显示rest API server request

// 画图表示, gcloud, Google Cloud API; Kubectl K8s APIServer (需要depend: k8s大致结构图)

## Deployment, Resource planning

Scenario: task to deploy the new Microservice to cloud

*  How many ec2 instance you need?
*  How should you arrange the service in different ec2(nodes)?

![20200614133437](https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20200614133437.png)

Containerize, resource planning

## Problem abstract:

![20200614133215](https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20200614133215.png)

// real resource request/limit
*  Tile CPU 2.5/3.5 13G/14G
*  Coverage 1.5/6.5 8G/20G
*  Area 100m/100Mi; 250m/250Mi

## Do it manually

*  Randomly pick one node that meet resource requirement
*  Good to have same service instance on different nodes.

//　手动schedule的结果

## Proxy

Create a proxy to remote Pods
// Kubectl proxy save your trouble to put request credential because kubectl already has it.
![20200705211742](https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20200705211742.png)

## K8s Resource plan, Schedule Demo

// k8s 自动部署, pod by pod, 先别考虑replica, replica放在resilience中. 

// run 3 pods
```
kubectl run --restart=Never --image=gcr.io/kuar-demo/kuard-amd64:green
```

```
kubectl proxy
docker run -it --net=host hjacobs/kube-ops-view
```

## How k8s handle Resource go over limit
// Let’s imagine a scenario where we have a machine that is running out of memory. What will Kubernetes do?

//  If your Pod’s containers have no requests, then by default they are using more than they requested, so these are prime candidates for termination
// Other prime candidates are containers that have gone over their request but are still under their limit.
// If Kubernetes finds multiple pods that have gone over their requests, it will then rank these by the Pod’s priority, and terminate the lowest priority pods first.

https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-resource-requests-and-limits

## High Availability

// In last section, Pods are essentially one-off singletons
// New challenges: pod crashing, node down; 
We want to avoid: down time.

[Nearmap status](https://status.nearmap.com/)

*  What if application crashed?
*  What if node(s) down?

Essential problem: How to avoid SPOF(Single point of failure)?

Need several identical pods working together: replicas

// For example, we want it always be 3 nodes running
ReplicaSet: create a group of exactly same pods

## ReplicaSet

.code resource/rs.yaml

## Demo

```
kubectl get pods
kubectl delete pod kuard-lfwjq
kctl delete pods -l app=kuard
```

// Controller loop: non-terminating loop that regulates the state of a system
// Object have a spec field that represents the desired state

## Relicateset review

*  Redundancy: Multiple running instances mean failure can be tolerated.
*  Scale: Multiple running instances mean that more requests can be handled.

## Another view of K8s: the object store

*  Most concept has an underlying object, defined by yaml: CRUD
*  Declarative
*  Loose couple

## Service Discovery, load balancing

// 在Resilience之后　Resilience引出replica, 才能many to many 

Service discovery is the process of automatically detecting devices and services on a network

// When you break down your monolithic application to several focused microservices, you will have to find an efficient way to locate your services

![20200614192449](https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20200614192449.png)

Solve the many to find many problem

## Naive solution

Problem: Group of As need access group of B

Hardcode all IP address, downside

*  Couple between A and B's IP(very bad)
*  Need to change config if B added new instance, remove instance
*  Didn't do liveness check of B

// Need to restart A after changes in B

## How we solve it in K8s

Essential problem: pick anyone from a group

1. Each pod will have a label
2. Use label selector to specify the group of pods you wanted

```
kubectl create namespace k8s-talks
kubectl config set-context --current --namespace=k8s-talks
kubectl run --restart=Never -l app=k8s-talks --image=gcr.io/kuar-demo/kuard-amd64:green k8s-talks-1
kubectl run --restart=Never -l app=k8s-talks --image=gcr.io/kuar-demo/kuard-amd64:green k8s-talks-2
// kubectl delete all --all -n k8s-talks
```

## K8s Service

.code resource/service.yaml


## K8s Service Demo

```
kubectl apply -f resource/service.yaml
kubectl get svc
```

Verify if we can randomly get pods `belong` to service(i.e: group)

// 用curl 验证, 因为browser keep alive, 服用同一个connection, 不会随机选pod
// 观察curl得到的 hostname, 为pod name.
```
curl http://34.87.250.164/
```

## Access k8s service internally

Access by ClusterIP, or, better by DNS

```
NAME        TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)        AGE
k8s-talks   LoadBalancer   x.x.x.x       x.x.x.x        80:30957/TCP   12m
```

Go to pod demo page: 

DNS query: A record, name: k8s-talks
Get a stable IP: 

```
;; ANSWER SECTION:
k8s-talks.k8s-talks.svc.cluster.local.	28	IN	A	10.3.252.70
```

## Release new version

Now you have a app running in K8s: 

*  Backed by ReplicateSet, say 3 replicas(pods). 
*  Service object point to 3 pods.

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201021092340.png" alt="20201021092340" style="width:450px"/>

You have a new version, how you gonna release it?

// Beijing Udesk story: 

## Old ways

*  'Stone age': tar all binary and config, stop, replace, restart
*  Semi automated tools: Ansible

Imperative, a lot of human intervention
*  Tar related files
*  Copy to all hosts
*  Stop, replace, restart

// Things works in test not always in prod
// boring, risky, important event, make people nervous
// Can miss some, at some time, or some instance unhealthy.

// Release in Amap: provide map service in China, long list of hosts, shell script to release, silently fail, inconsistent version.

## K8s way

// very typical tech scenario, one of big reason I like k8s so much, my own story, last job
With K8s(Deployment object), we can achieve it by changing one field(image url), k8s will do the rest.

With k8s, to release:

*  Build a docker image of new version, push it to registry.
*  Change image url field of Deployment object

You get zero downtime release(also depend on your app)

## 3 different deployment strategies

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201021092340.png" alt="20201021092340" style="width:450px"/>

Both these strategies have their benefits and drawbacks: 

*  Remove old pods, then create new pods: Down time
*  Rolling update(our focus): Start new ones, then delete old ones: must support running two versions at the same time
*  Blue-green: create new ones, switch to new ones at once, then delete old ones

// Idea behind deployment object
// One of favourite feature in k8s early days
// We could say there's a new object controlling the deployment, but better to discuss reason why we need additional object.

## Deleting old and replacing with new

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201021101741.png" alt="20201021101741" style="width:1000px"/>

// Update pod template: v2
// Delete all current pods(v1)

## Rolling update

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201021102401.png" alt="20201021102401" style="width:1000px"/>

## Blue-green

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201021103223.png" alt="20201021103223" style="width:1000px"/>

## Doing Rolling Update manually

1.  Manually create ReplicaSet with new Pod version
2.  Resize both as we go: old--, new++

Pseudo code for resize logic:  

.code resource/sample/pseudo_rs.go

You need kubectl extensively

Doing a rolling update manually is laborious and error-prone

## k8s Deployment Object

A Deployment is a higher-level resource meant for deploying applications and updating them declaratively

Deployment(high level) object builds on RelicaSet(low level).

We need a high level object coordinate 2 RelicaSet(downsize old and upsize new gradually)

Make things much easier: just declare the desired state, k8s will do it for you.

// think like a manager:you worker: do the stuff

## Create a Deployment

.code resource/svc-deployment-v1.yaml /START OMIT/,/END OMIT/

.code resource/sample/deploy_v2.sh

## Scaling in K8s

// another interesting topic, distinguish between toy env and real env, vital to prod env
// So easy with k8s, config and forget it

What we've got now: 

With RelicaSet, you can manually scale

```
kubectl scale rs-name --replicas=10 
```

Not ideal, what we really want: scale based on work load.

// Mannully scale is what we did in early age, predict a spike, then allocate instance beforehead.
3 types of scaling:

*  Horizontal pod autoscaling
*  Vertical pod autoscaling: e.g scale db, not work horizonly
*  Cluster node autoscaling

// lift box to upper level;  stronger worker or more worker.
// Cluster autoscaling: build new office for more people.

## Horizontal pod autoscaling demo

// Use kube ops view to demo
// Visually see pod scaling before explaining

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201022135935.png" alt="20201022135935" style="width:800px"/>

## Horizontal pod autoscaling

Automatic scaling of the number of pod replicas based on some metric: CPU, memory, custom.

// 3 worker(replica), work increase(request), more worker(pod)

Controlled by the HorizontalPodAutoscaler (HPA) resource:

*  HPA defined expected average CPU usage of pods belong to target resource: ReplicaSet, Deployment
*  Periodically checks pods metric: CPU usage
*  Calculate pods count: podsCount = SumOfCPU / target CPU
*  Change replica count to make pods count close to target value

// also makes sure the Autoscaler doesn’t thrash around when the metric value is unstable and changes rapidly.

## What exactly CPU usage means

Node's CPU? Pod's guaranteed CPU? Pod's hard limit CPU?

*  CPU usage: [0, 1], in HPA, means the pod’s guaranteed CPU amount: CPU request
*  Not above 0.9 to leave enough room for sudden request spike

The Autoscaler compares the pod’s actual CPU consumption and its CPU requests

## HPA object

.code resource/hpa_areas.yaml

Value could be CPU core number: 

e.g expected 50%

*  Current 100%, resizeRatio = 100 / 50 = 2
*  Current 10%, resizeRatio = 10 / 50 = 0.2

## HPA calculate new pods count

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201022092951.png" alt="20201022092951" style="width:1000px"/>

## Simplified picture

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201022093927.png" alt="20201022093927" style="width:1000px"/> 

## Big picture

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201022094006.png" alt="20201022094006" style="width:1000px"/>

// feedback loop: pod CPU metric -> HPA -> action to make it close to expected metric

## Demo

// before or after explaination
// We had a deployment object(default 3 replica running):
.code resource/svc-deployment-v1.yaml /START OMIT/,/END OMIT/

```
kubectl apply -f resource/svc-deployment-v1.yaml
```

## Create HPA 

```
kubectl autoscale -f resource/hpa_talks.yaml
```

Start load test:

```
wrk -t 5 -c5 -d3000s -H 'Host: example.com' -R 200 --timeout 2s http://35.189.39.71/
```

View result
```
watch -n 5 kubectl get hpa,deployment
kubectl describe hpa
```

// take 6 minutes to down scale 

What about your nodes are full because of HPA? 

Is there something can automatically spinup new nodes

// cmd should enable before? after create cluster? can enable when creating it?

## Cluster Autoscaler

// Based on node's metric, HPA based on Pod's metric(if a Pod gets too busy, spinup more pods).
// K8s support it, but also depend on cloud or on premise

*  Automatically request additional nodes from the cloud provider
*  Scales up when it finds a pod that can’t be scheduled to existing nodes
*  De-provisions nodes when they’re underutilized for longer periods of time

Add-on of K8s(no object needed):

In our kepler k8s cluster:
```
kubectl get pods -n kube-system | grep -i cluster-autoscale
```

we got: 

```
cluster-autoscaler-66d767794b-xzkk8
```

// downscale take more time, happens after HPA to scale down first.
// just leave it there see if can happen before our session ends.

Enable cluster auto scaler in GKE
```
gcloud container clusters update k8s-talks --enable-autoscaling --min-nodes=3 --max-nodes=5
```

## Cluster Autoscaler

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201022143214.png" alt="20201022143214" style="width:1000px"/>

// Just give the demo: k8s can do it. config more on cloud provider's side

## Demo

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201022142041.png" alt="20201022142041" style="width:1000px"/>

// view node resource
// kubectl top nodes
// kubectl describe nodes

## What's next?

*  Online playground k8s
*  K8s up and running: take advantage of GCP,  & Azure, download book for free
*  K8s in action

API team channel support?

## Reference

.link https://stackoverflow.com/questions/31932945/does-go-have-something-like-threadlocal-from-java Does Go have something like ThreadLocal from Java?  
