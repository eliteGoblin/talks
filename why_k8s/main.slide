# Why kubernetes

a beginner's guide from microservice perspective  

23 Oct 2020

Frank Sun
Golang Developer, API Team
elitegoblinrb@gmail.com

## About this session

Goal:

*  Explain why k8s in so useful in building microservice
// As discussed with Simon, we want to bring more people's intrest into using k8s if they havn't do it yet. 
// How individual will benefit from using it, save us from a lot a tedious work, also system more resilient; as a result, Nearmap will also benefit from it, increased performance and less service disruption.
// Not just for microservice, we use it as a concrete example, as a goal, should help understand it. Better than just talk abstractly its benefit
// Adam asked what k8s do and why we need it, I mentioned pod shceduling, not quite convicing.
*  Get beginners somehow onboard with k8s
// a couple of live demo contained
// We are considering provide support while you learn and start to use k8s?

## A bit about myself

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201023144634.png" alt="20201023144634" style="width:850px"/>

Tech stacks:

*  Alibaba: c++, Linux as pet
*  Udesk: golang, etcd, Nginx/OpenResty, AliCloud, Ansible
*  Nearmap: golang, k8s, AWS(terraform), Argo...

// In these 3 job arcoss 3 phase
// monilith -> microservice: understand why and how to build microservice with a collection of opensource tools
// Microservice on K8s: realized that before so much of my time wasted not using k8s, 
//   1. No need to build our own service discovery infra(etcd) and related code.
//   2. Release be so much easier, no downtime, release anytime.

## Roadmap

*  Brife history & background of k8s
// k8s build on docker, solve microservice problems, we need to know docker, k8s, and microservice; each itself is a big topic
// Goal is understand what problems to solve, then understand why k8s
*  Why Microservice
// Microservice is our goal to build, and example, give us a real scenario helps us understand
*  Why Docker
// K8s build on docker, why the low level unit is Pod
*  How k8s helps building Microservice, contains
  
1.  Schedule workload over cluster nodes, automatically
2.  Self-healing, resilience
// survive app crash, node failure
// when spike of request coming
3.  Service discovery & Load balancing
4.  Deploy new version
// not build in CI/CD solution, deployment support, not hard to implement different deployment strategy, blue-green, canary, ...
5.  Linearly scaling with workload automatically

## What's not covered

*  K8s objects other than: Pod, RelicaSet, Service, Deployment, HPA
*  K8s internals, could be

    1.  controller loop
    2.  Service internal, CoreDNS 
    3.  HA clusters
    4.  pod network
    ...

## What is Kubernetes

Kubernetes (“koo-burr-NET-eez”) is originated from a Greek word, κυβερνήτης, meaning “helmsman” or “pilot.” 

<img src="https://miro.medium.com/max/1225/1*6NVMEmo0qDcQjpXcXI8dtg.png" alt="drawing" width="600"/>

## Definitions of Kubernetes

// First we might want to try from get What is k8s from its definition

Wikipedia    

> An open-source container-orchestration system for automating application deployment, scaling, and management.  It aims to provide a "platform for automating deployment, scaling, and operations of application containers across clusters of hosts"

Kubernetes.io    

> Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. 

## What is Kubernetes

Key words: 

- Container
- Orchestration, managing
- Automation 
- Scaling, Auto heal

Kubernetes is the your computing cluster's operating system.

// Analogy, let's see the different with traditional OS

## Compared to Linux as an OS

OS manages computer low-level resources, provides upper level for computer programs.

Linux manage single machine resources: CPU, memory; Disk, IO device; Network device

*  Provide service in application level: Process/thread scheduling, memory management; Socket; File service;
*  Developer interface: Linux API

K8s manage resource across multiple nodes(e.g 3 nodes, 10CPU, 43G mem)
*  Features provided: Workload scheduleing(both CPU & memory), ReplicaControl, Service Discovery, Pod Virtual Network, Deployment, Storage 
*  Application interface: API server


## Kubernetes as cluster OS

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201023165424.png" alt="20201023165424" style="width:850px"/>

## Kubernetes as cluster OS

K8s as OS provide useful API for building microservice system on cluster: 

- **Workloads scheduling**
- **Self-healing, Scaling**
- **Service discovery, Load balancing**
- **Automated rollouts and rollbacks**
- Storage orchestration
- Secret and configuration management

K8s makes building Microservice much easier! 

// Productivity, Small group of people can manage large cluster

## Kubernetes terminology

We need to talk in K8s terminology: 

- Node: a single computing instance
- **Pod**: basic deploy/running unit, like process in Linux
- **Replicate Set**: group of identical pods(replicas)
- **Service**: reverse proxy/load balancer, group your pod, like Nginx 
- **Deployments**: rolling update 
- Daemon Set
- Job
- Secrets and configMap
- Volume

// Pod (unbreakable unit when deploy and run), can think of equal contfainer
// It provide useful functions when building micro-service

## K8s architecture

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201023165424.png" alt="20201023165424" style="width:850px"/>

// Human use API or kubectl to access apiserver; User access service via IP addr
// ETCD: persist state data, create whole cluster with Phonebook of all services(imagine you lost all your contact, WFH and slack down)

## K8s History and Jobs Trend

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201024113730.png" alt="20201024113730" style="width:800px"/>

*  Initial release: 7  June 2014
*  v1.0:            10 July 2015

Original author: Google, heavily influenced by its Borg system

## K8s Jobs Trend

Some data from internet:

Between October 2015 and October 2019, the share of Kubernetes job searches increased by **2,125%**.  Kubernetes-related jobs per million also grew by **2,141%** in that same time period,

Also interesting, LinkedIn 2020 Emerging Jobs Report:

[US](https://business.linkedin.com/content/dam/me/business/en-us/talent-solutions/emerging-jobs-report/Emerging_Jobs_Report_U.S._FINAL.pdf)  
[AU](https://business.linkedin.com/content/dam/me/business/en-us/talent-solutions/emerging-jobs-report/AUS-TOP-EMERGING-JOBS_compressedRevised.pdf)

SRE ranked 5th in Both AU and US, Requirement

## K8s Jobs Trend continued

*  US: AWS, Ansible, Kubernetes, Docker, Terraform
*  AU: AWS, Linux, Go, DevOps, Docker

From LinkedIn job search trend: 

From 2018 to 2019, the share of Kubernetes jobs per million rose 53.33%  
From 2018 to 2019, the share of Kubernetes job searches saw a mere .85% gain.

Seems still a seller's market 

## Microservice, and why

Microservice is an architectural style that structures an application as a collection of services working together.

// We need to first understand challenges in microservice then can understand how to compare solution with/without k8s

A distributed system, collection of service which is small, and focused on **Doing One Thing Well**

Benefit: decouple 
// Quality of architecture is how good you decouple your system

- Scaling separately; Monolithic service has to scale everything together
- Deployment separately
- Resilience: failure doesn’t cascade, gracefully degradation(e.g. when suggestion service down, still get most popular items)
- Technology Heterogeneity
- Optimizing for Replaceability/Refactoring

Organizational Alignment: smaller team to manage smaller service; 

// Conway's law: Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization's communication structure.

// e.g: US only need to scale sales/marketing, not much engineer/tech

## Challenges of Microservice

Mainly come from: we break a system to many pieces. 

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201024121824.png" alt="20201024121824" style="width:200px"/> 

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201024121958.png" alt="20201024121958" style="width:200px"/>

## Challenges of Microservice

- Resource planning
- High availability(HA): pod fail, node fail
- Scaling linearly with workloads
- Service Discovery
- Release new version: Automated rollouts and rollbacks with zero-downtime

## We build Microservice in API team

1.  coverage
2.  tiles
3.  sourcephotos  
  
Internally:  


1.  area
2.  transaction
3.  tilesprocess

Demo: what happen behind the scence when user open mapbrowser

## Containerize and docker

Running software in containers for isolation:

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201024122552.png" alt="20201024122552" style="width:900px"/>

## Benefit

- **Self-contained, isolation**: isolated process and network
- **Resource requirement**
- Lightweight
- Portable

Container images bundle a program and its dependencies into a single artifact;
Build once, running everywhere.

//  all of these programs share the same versions of shared libraries on the system
// Isolated process in Linux

// Can just work in a total different environment

// Think it's a light weighted VM, cgroup

// everbright bank story: depend on same version of redis-cli, sql driver
// Isolation: share house and studio, buckhead

## Why named Pod?

Pod: a social group of whales

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201024122936.png" alt="20201024122936" style="width:250px"/>

Docker container: Whale

<img src="https://cdn.vox-cdn.com/thumbor/YiSPuNNVjUhKnYPpqzVinwYCRK8=/1400x1400/filters:format(png)/cdn.vox-cdn.com/uploads/chorus_asset/file/11422405/Docker_logo_011.png" alt="drawing" width="250"/>

Pod = Group of whales = Group of docker container 

## Demo environment

*  K8s Platform: Google Kubernetes Engine(GKE)
*  Client Tools: gcloud, kubectl(binaries)

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201023165424.png" alt="20201024122936" style="width:500px"/>

## Init demo environment

Create k8s cluster in GKE:

```
gcloud init --skip-diagnostics
gcloud config set compute/region australia-southeast1
gcloud config set compute/zone australia-southeast1-a
gcloud container clusters create k8s-talks --num-nodes=3 --enable-autoscaling --min-nodes=3 --max-nodes=3
```

Use kubectl to test if cluster up

config file in `$HOME/.kube/config`
```
gcloud container clusters get-credentials k8s-talks
kubectl config get-contexts
kubectl get nodes -v 6
```

## Deployment, Resource planning

Scenario: task to deploy the new Microservice to cloud.   
Services and replicas: A(3), B(5), C(2), D(2)

*  How many ec2 instance you need?
*  How should you arrange the service in different ec2(nodes)?

What's your Microservice finally looks like on cloud after deployment?

## Resource planning

How you gonna put your stuff(service) in to box(computing instance)?

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201031143738.png" alt="20201031143738" style="width:800px"/>

## Example from API team

// real resource request/limit
*  Tile CPU 2.5/3.5 13G/14G, 3 replicas
*  Coverage 1.5/6.5 8G/20G, 2 replicas
*  Area 100m/100Mi; 250m/250Mi, 4 replicas

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201031144322.png" alt="20201031144322" style="width:500px"/>

## Do it manually (Consider Memory only)

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201031145507.png" alt="20201031145507" style="width:600px"/>

*  Randomly pick one node that meet resource requirement
*  Good to have same service instance on different nodes.

## Cons of manually scheduling

*  Tedious, consider 200+ service replicas, need a intelligent algorithm to help
*  Hard to keep track of real state of deploying: different nodes has different state, pets
*  Very difficult to cope with changing load

## K8s Resource plan, Schedule Demo

// k8s 自动部署, pod by pod, 先别考虑replica, replica放在resilience中. 

deploy 3 pods
```
kubectl run --restart=Never --image=gcr.io/kuar-demo/kuard-amd64:green
```

```
kubectl proxy
docker run -it --net=host hjacobs/kube-ops-view
```

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201031145645.png" alt="20201031145645" style="width:500px"/>

// Possible question: what if Pod not specify limit and How k8s handle Resource go over limit
// Let’s imagine a scenario where we have a machine that is running out of memory. What will Kubernetes do?

//  If your Pod’s containers have no requests, then by default they are using more than they requested, so these are prime candidates for termination
// Other prime candidates are containers that have gone over their request but are still under their limit.
// If Kubernetes finds multiple pods that have gone over their requests, it will then rank these by the Pod’s priority, and terminate the lowest priority pods first.

// https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-resource-requests-and-limits

## High Availability

// In last section, Pods are essentially one-off singletons
// New challenges: pod crashing, node down; 
We want to avoid: down time.

[Nearmap status](https://status.nearmap.com/)

*  What if application crashed?
*  What if node(s) down?

Essential problem: How to avoid SPOF(Single point of failure)?

Need several identical pods working together: replicas

// For example, we want it always be 3 nodes running
ReplicaSet: create a group of exactly same pods

## ReplicaSet

.code resource/rs.yaml

## Demo

```
kubectl get pods
kubectl delete pod kuard-lfwjq
kctl delete pods -l app=kuard
kubectl scale --replicas=5 rs/kuard
```

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201031145645.png" alt="20201031145645" style="width:500px"/>

// Controller loop: non-terminating loop that regulates the state of a system
// Object have a spec field that represents the desired state

// teardown 1 node, see what happens, ssh/browser into GCP node and restart/teardown

## Relicateset review

*  Redundancy: Multiple running instances mean more failure can be tolerated.
*  Scale: Multiple running instances mean that more requests can be handled.

## Another view of K8s: the object store

*  Most concept has an underlying object, defined by yaml
*  Declarative
*  Different object to address different problem, some are build based on another.

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201024124852.png" alt="20201024124852" style="width:300px"/>

K8s terms we mentioned in this session all have corresponding object stored in K8s.

## Service Discovery

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201031151053.png" alt="20201031151053" style="width:550px"/>

## Service Discovery

Service discovery is the process of automatically detecting devices and services on a network

When break down your monolithic application to several focused microservices, what's the efficient way to locate your services?

Underlying problem: retrieve any one from a group.

## Naive solution

Problem: Group of As need access group of B

Hardcode all IP address, downside

*  Couple between A and B's IP(very bad)
*  Need to change config if B added new instance, remove instance
*  Didn't do liveness check of B

// Need to restart A after changes in B

## Better solution: Order Uber eats

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201031151437.png" alt="20201031151437" style="width:500px"/>

*  We don't know any of delivery guy
*  How we choose one deliver food for us?

We need a service registry: who know all delivery guys working ATM.

## Server side discovery

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201031151804.png" alt="20201031151804" style="width:800px"/>

Kubernetes adopt this idea.

## Client side discovery

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201031152034.png" alt="20201031152034" style="width:700px"/>

// https://searchapparchitecture.techtarget.com/tip/Fundamental-patterns-for-service-discovery-in-microservices

Client side discovery means Uber eats give us list of dilivery guys's phone number, we need to call one of them.

## How we solve it in K8s

Essential problem: pick anyone from a group

1. Each pod will have a label
2. Use label selector to specify the group of pods you wanted

```
kubectl create namespace k8s-talks
kubectl config set-context --current --namespace=k8s-talks
kubectl run --restart=Never -l app=k8s-talks --image=gcr.io/kuar-demo/kuard-amd64:green k8s-talks-1
kubectl run --restart=Never -l app=k8s-talks --image=gcr.io/kuar-demo/kuard-amd64:green k8s-talks-2
// kubectl delete all --all -n k8s-talks
```

## K8s Service

.code resource/service.yaml

## K8s Service Demo

```
kubectl apply -f resource/service.yaml
kubectl get svc
```

Verify if we can randomly get pods `belong` to service(i.e: group)

// 用curl 验证, 因为browser keep alive, 服用同一个connection, 不会随机选pod
// 观察curl得到的 hostname, 为pod name.
```
curl http://34.87.250.164/
```

## Access k8s service internally

Access by ClusterIP, or, better by DNS

```
NAME        TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)        AGE
k8s-talks   LoadBalancer   x.x.x.x       x.x.x.x        80:30957/TCP   12m
```

Go to pod demo page: 

DNS query: A record, name: k8s-talks
Get a stable IP: 

```
;; ANSWER SECTION:
k8s-talks.k8s-talks.svc.cluster.local.	28	IN	A	10.3.252.70
```

## Deploy new version

Now you have a app running in K8s: 

*  Backed by ReplicateSet, say 3 replicas(pods). 
*  Service object point to 3 pods.

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201021092340.png" alt="20201021092340" style="width:450px"/>

You have a new version, how you gonna release it?

// Beijing Udesk story: 

## Old ways

*  'Stone age': tar all binary and config, stop, replace, restart
*  Semi automated tools: Ansible

Imperative, a lot of human intervention
*  Tar related files
*  Copy to all hosts
*  Stop, replace, restart

// Things works in test not always in prod
// boring, risky, important event, make people nervous
// Can miss some, at some time, or some instance unhealthy.

// Release in Amap: provide map service in China, long list of hosts, shell script to release, silently fail, inconsistent version.

## K8s way

// very typical tech scenario, one of big reason I like k8s so much, my own story, last job
With K8s(Deployment object), we can achieve it by changing one field(image url), k8s will do the rest.

With k8s, to release:

*  Build a docker image of new version, push it to registry.
*  Change image url field of Deployment object

You get zero downtime release(also depend on your app)

## 3 different deployment strategies

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201021092340.png" alt="20201021092340" style="width:450px"/>

Both these strategies have their benefits and drawbacks: 

*  Remove old pods, then create new pods: Down time
*  Rolling update(our focus): Start new ones, then delete old ones: must support running two versions at the same time
*  Blue-green: create new ones, switch to new ones at once, then delete old ones

// Idea behind deployment object
// One of favourite feature in k8s early days
// We could say there's a new object controlling the deployment, but better to discuss reason why we need additional object.

## Deleting old and replacing with new

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201021101741.png" alt="20201021101741" style="width:1000px"/>

// Update pod template: v2
// Delete all current pods(v1)

## Rolling update

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201021102401.png" alt="20201021102401" style="width:1000px"/>

## Blue-green

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201021103223.png" alt="20201021103223" style="width:1000px"/>

## Doing Rolling Update manually

1.  Manually create ReplicaSet with new Pod version
2.  Resize both as we go: old--, new++

Pseudo code for resize logic:  

.code resource/sample/pseudo_rs.go

You need kubectl extensively

Doing a rolling update manually is laborious and error-prone

## k8s Deployment Object

A Deployment is a higher-level resource meant for deploying applications and updating them declaratively

Deployment(high level) object builds on RelicaSet(low level).

We need a high level object coordinate 2 RelicaSet(downsize old and upsize new gradually)

Make things much easier: just declare the desired state, k8s will do it for you.

// think like a manager:you worker: do the stuff

## Create a Deployment

.code resource/svc-deployment-v1.yaml /START OMIT/,/END OMIT/

.code resource/sample/deploy_v2.sh

## Scaling in K8s

// another interesting topic, distinguish between toy env and real env, vital to prod env
// So easy with k8s, config and forget it

What we've got now: 

With RelicaSet, you can manually scale

```
kubectl scale rs-name --replicas=10 
```

Not ideal, what we really want: scale based on work load.

// Mannully scale is what we did in early age, predict a spike, then allocate instance beforehead.
3 types of scaling:

*  Horizontal pod autoscaling
*  Vertical pod autoscaling: e.g scale db, not work horizonly
*  Cluster node autoscaling

// lift box to upper level;  stronger worker or more worker.
// Cluster autoscaling: build new office for more people.

## Horizontal pod autoscaling demo

// Use kube ops view to demo
// Visually see pod scaling before explaining

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201022135935.png" alt="20201022135935" style="width:800px"/>

## Horizontal pod autoscaling

Automatic scaling of the number of pod replicas based on some metric: CPU, memory, custom.

// 3 worker(replica), work increase(request), more worker(pod)

Controlled by the HorizontalPodAutoscaler (HPA) resource:

*  HPA defined expected average CPU usage of pods belong to target resource: ReplicaSet, Deployment
*  Periodically checks pods metric: CPU usage
*  Calculate pods count: podsCount = SumOfCPU / target CPU
*  Change replica count to make pods count close to target value

// also makes sure the Autoscaler doesn’t thrash around when the metric value is unstable and changes rapidly.

## What exactly CPU usage means

Node's CPU? Pod's guaranteed CPU? Pod's hard limit CPU?

*  CPU usage: [0, 1], in HPA, means the pod’s guaranteed CPU amount: CPU request
*  Not above 0.9 to leave enough room for sudden request spike

The Autoscaler compares the pod’s actual CPU consumption and its CPU requests

## HPA object

.code resource/hpa_areas.yaml

Value could be CPU core number: 

e.g expected 50%

*  Current 100%, resizeRatio = 100 / 50 = 2
*  Current 10%, resizeRatio = 10 / 50 = 0.2

## HPA calculate new pods count

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201022092951.png" alt="20201022092951" style="width:1000px"/>

## Simplified picture

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201022093927.png" alt="20201022093927" style="width:1000px"/> 

## Big picture

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201022094006.png" alt="20201022094006" style="width:1000px"/>

// feedback loop: pod CPU metric -> HPA -> action to make it close to expected metric

## Demo

// before or after explaination
// We had a deployment object(default 3 replica running):
.code resource/svc-deployment-v1.yaml /START OMIT/,/END OMIT/

```
kubectl apply -f resource/svc-deployment-v1.yaml
```

## Create HPA 

```
kubectl autoscale -f resource/hpa_talks.yaml
```

Start load test:

```
wrk -t 5 -c5 -d3000s -H 'Host: example.com' -R 200 --timeout 2s http://35.189.39.71/
```

View result
```
watch -n 5 kubectl get hpa,deployment
kubectl describe hpa
```

// take 6 minutes to down scale 

What about your nodes are full because of HPA? 

Is there something can automatically spinup new nodes

// cmd should enable before? after create cluster? can enable when creating it?

## Cluster Autoscaler

// Based on node's metric, HPA based on Pod's metric(if a Pod gets too busy, spinup more pods).
// K8s support it, but also depend on cloud or on premise

*  Automatically request additional nodes from the cloud provider
*  Scales up when it finds a pod that can’t be scheduled to existing nodes
*  De-provisions nodes when they’re underutilized for longer periods of time

Add-on of K8s(no object needed):

In our kepler k8s cluster:
```
kubectl get pods -n kube-system | grep -i cluster-autoscale
```

we got: 

```
cluster-autoscaler-66d767794b-xzkk8
```

// downscale take more time, happens after HPA to scale down first.
// just leave it there see if can happen before our session ends.

Enable cluster auto scaler in GKE
```
gcloud container clusters update k8s-talks --enable-autoscaling --min-nodes=3 --max-nodes=5
```

## Cluster Autoscaler

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201022143214.png" alt="20201022143214" style="width:1000px"/>

// Just give the demo: k8s can do it. config more on cloud provider's side

## Demo

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201022142041.png" alt="20201022142041" style="width:1000px"/>

// view node resource
// kubectl top nodes
// kubectl describe nodes

## What's next?

##  Learn Kubernetes Basics(Official Interactive Tutorial)

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201031152739.png" alt="20201031152739" style="width:800px"/>

https://kubernetes.io/docs/tutorials/kubernetes-basics/

## K8s up and running, 2rd

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201031152920.png" alt="20201031152920" style="width:350px"/>

## K8s in action

<img src="https://raw.githubusercontent.com/eliteGoblin/images/master/blog/img/picgo/20201031153045.png" alt="20201031153045" style="width:350px"/>

Best K8s book(2rd is coming)
